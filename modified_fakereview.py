# -*- coding: utf-8 -*-
"""Modified_FakeReview.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MA3sp43a5duMlKsc8J5TOCVgViTFh8j3
"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
!pip install contractions

import numpy as np
import tensorflow as tf
import sklearn as sk
import matplotlib.pyplot as plt
import pandas as pd
import io
import contractions
import nltk
import string
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer
from google.colab import files
import csv

"""Load the Dataset"""

review_data = pd.read_csv('fake_reviews_dataset.csv')
review_data

train_data, test_data = train_test_split(review_data, test_size=0.2)

print('Training data shape:', train_data.shape)
print('Test data shape:', test_data.shape)

review_data.loc[review_data['label'] == 'CG', 'Label'] = '1'
review_data.loc[review_data['label'] == 'OR', 'Label'] = '0'

"""**Remove Nulls**"""

for col in review_data.columns:
  print(col, review_data[col].isnull().sum())

"""**Tokenization**"""

review_data['tokenized'] = review_data['text_'].apply(word_tokenize)
review_data

"""**Convert tokenization to lowercase**"""

review_data['lowercase'] = review_data['tokenized'].apply(lambda x: [word.lower() for word in x])
review_data

"""**Removal of punctuation**"""

punc = string.punctuation
review_data['remove_punc'] = review_data['lowercase'].apply(lambda x: [word for word in x if word not in punc])
review_data

"""**Removing Stop Words**"""

stop_words = set(stopwords.words('english'))
review_data['remove_stopwords'] = review_data['remove_punc'].apply(lambda x: [word for word in x if word not in stop_words])
review_data

"""**Perform Speech Tagging**"""

# Parts of speech tagging of word vectors

review_data['speech_tagging'] = review_data['remove_stopwords'].apply(nltk.tag.pos_tag)

def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN
    
    
    
    
review_data['wordnet_pos'] = review_data['speech_tagging'].apply(lambda x: [(word, get_wordnet_pos(speech_tagging)) for (word, speech_tagging) in x])
review_data

"""**Lemmatization**"""

wnl = WordNetLemmatizer()
review_data['lemmatized'] = review_data['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])
review_data

"""**Sentiment Analysis using Distilbert Model**"""

!pip install transformers
!pip install scipy

import transformers

review_data['lemmatized_str_form'] = [' '.join(map(str, word)) for word in review_data['lemmatized']]

review_data['lemmatized_str_form']

# from transformers import pipeline
# sentiment_pipeline = pipeline("sentiment-analysis")
# count = 0
# data_transform = review_data['lemmatized_str_form']
# for i in data_transform:
#   count +=1
#   transform_data = sentiment_pipeline(i)
#   #print(transform_data)

# #print(count)

"""**Sentiment Analysis using Vader**"""

nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()

vader_score = []
vader_class = []

for review in review_data['lemmatized_str_form']:
    ss = sid.polarity_scores(review)
    compound_score = ss.get('compound')
    vader_score.append(compound_score)
    if (compound_score >= 0):
        vader_class.append(1)
    else:
        vader_class.append(0)

review_data['vader_score'] = vader_score
review_data['vader_class'] = vader_class

review_data

"""**Fake Review Detection**"""

review_data['Label'] =review_data['Label'].replace('1', 1)
review_data['Label'] =review_data['Label'].replace('0', 0)

# Check the ratio of number of fake vs non-fake reviews
num_fake_reviews = (review_data['Label'] == 1).sum()
num_original_reviews = (review_data['Label'] == 1).sum()
print('Number of fake reviews:', num_fake_reviews)
print('Number of original reviews:', num_original_reviews)

fd = review_data[review_data['Label'] == 1]
td= review_data[review_data['Label'] == 0]
plt.figure(figsize=(50,30))
plt.margins(0.02)
plt.xlabel('Sentiment', fontsize=50)
plt.xticks(fontsize=40)
plt.ylabel('Frequency', fontsize=50)
plt.yticks(fontsize=40)
plt.hist(fd['vader_score'], bins=50, label='Fake review', alpha=0.5)
plt.hist(td['vader_score'], bins=50, label = 'Not Fake Review', alpha=0.5)
plt.title('Sentiment Distribution for FAKE and Not-Fake Reviews', fontsize=60)
plt.grid()
plt.legend(fontsize=60)
plt.show()

"""#Testing which model is suitable for us"""

from sklearn.svm import LinearSVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import ExtraTreeClassifier
from sklearn.linear_model import RidgeClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import MultinomialNB
import time
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import cross_val_score

X = review_data['lemmatized_str_form']
y = review_data['Label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, shuffle=True)

classifiers = {}
classifiers.update({"XGBClassifier": XGBClassifier(eval_metric='logloss',
                                                   objective='binary:logistic',
                                                   )})
classifiers.update({"LinearSVC": LinearSVC()})
classifiers.update({"MultinomialNB": MultinomialNB()})
classifiers.update({"LGBMClassifier": LGBMClassifier()})
classifiers.update({"DecisionTreeClassifier": DecisionTreeClassifier()})
classifiers.update({"ExtraTreeClassifier": ExtraTreeClassifier()})
classifiers.update({"AdaBoostClassifier": AdaBoostClassifier()})
classifiers.update({"RidgeClassifier": RidgeClassifier()})
classifiers.update({"SGDClassifier": SGDClassifier()})
classifiers.update({"BernoulliNB": BernoulliNB()})

df_models = pd.DataFrame(columns=['model', 'run_time', 'roc_auc', 'roc_auc_std'])

for key in classifiers:
    
    start_time = time.time()
    pipeline = Pipeline([("tfidf", TfidfVectorizer()), ("clf", classifiers[key] )])
    cv = cross_val_score(pipeline, X, y, cv=5, scoring='roc_auc')

    row = {'model': key,
           'run_time': format(round((time.time() - start_time)/60,2)),
           'roc_auc': cv.mean(),
           'roc_auc_std': cv.std(),
    }
    
    df_models = df_models.append(row, ignore_index=True)
    
df_models = df_models.sort_values(by='roc_auc', ascending=False)

df_models

from sklearn.metrics import accuracy_score as acc_score, precision_score, recall_score, roc_auc_score

bundled_pipeline_SVC= Pipeline([("tfidf", TfidfVectorizer()), 
                             ("clf", LinearSVC())
                            ])
bundled_pipeline_SVC.fit(X_train, y_train)
y_pred_SVC = bundled_pipeline_SVC.predict(X_test)

accuracy_score_SVC = acc_score(y_test, y_pred_SVC)
precision_score_SVC = precision_score(y_test, y_pred_SVC)
recall_score_SVC = recall_score(y_test, y_pred_SVC)
roc_auc_score_SVC = roc_auc_score(y_test, y_pred_SVC)

print('Accuracy:', accuracy_score_SVC)
print('Precision:', precision_score_SVC)
print('Recall:', recall_score_SVC)
print('ROC/AUC:', roc_auc_score_SVC)

bundled_pipeline_SGD= Pipeline([("tfidf", TfidfVectorizer()), 
                             ("clf", SGDClassifier())
                            ])
bundled_pipeline_SGD.fit(X_train, y_train)
y_pred_SGD = bundled_pipeline_SGD.predict(X_test)

accuracy_score = acc_score(y_test, y_pred_SGD)
precision_score = precision_score(y_test, y_pred_SGD)
recall_score = recall_score(y_test, y_pred_SGD)
roc_auc_score = roc_auc_score(y_test, y_pred_SGD)

print('Accuracy:', accuracy_score)
print('Precision:', precision_score)
print('Recall:', recall_score)
print('ROC/AUC:', roc_auc_score)
